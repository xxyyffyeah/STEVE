import sys
import os

# ç¡®ä¿ä½¿ç”¨å½“å‰ç›®å½•çš„textgrad
current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, current_dir)

import textgrad as tg
from textgrad.tasks import load_task
from dotenv import load_dotenv

load_dotenv(override=True)

print("=== TextualAdamä¼˜åŒ–å™¨æ¼”ç¤º ===")
print("åŸºäºBBH object countingä»»åŠ¡çš„ç³»ç»Ÿæç¤ºä¼˜åŒ–\n")

# æ£€æŸ¥APIå¯†é’¥
if not os.getenv('OPENAI_API_KEY'):
    print("è­¦å‘Š: æœªæ£€æµ‹åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
    print("è¯·è®¾ç½®APIå¯†é’¥åé‡æ–°è¿è¡Œ")
    exit(1)

llm_engine = tg.get_engine("gpt-3.5-turbo-0125")
tg.set_backward_engine("gpt-4o")

train_set, val_set, test_set, eval_fn = load_task("BBH_object_counting", llm_engine)
question_str, answer_str = val_set[0]
question_str = "I have four cauliflowers, a garlic, a cabbage, a potato, a head of broccoli, three yams, a lettuce head, an onion, and a carrot. How many vegetables do I have?"
answer_str = "14"
question = tg.Variable(question_str, role_description="question to the LLM", requires_grad=False)
answer = tg.Variable(str(answer_str), role_description="answer to the question", requires_grad=False)

print(f"æµ‹è¯•é—®é¢˜: {question_str}")
print(f"æ­£ç¡®ç­”æ¡ˆ: {answer_str}\n")

STARTING_SYSTEM_PROMPT = train_set.get_task_description()

system_prompt = tg.Variable(STARTING_SYSTEM_PROMPT,
                            requires_grad=True,
                            role_description="system prompt to guide the LLM's reasoning strategy for accurate responses")

model = tg.BlackboxLLM(llm_engine, system_prompt=system_prompt)

# ä½¿ç”¨TextualAdamä¼˜åŒ–å™¨æ›¿ä»£TGD
optimizer = tg.TAdam(
    parameters=list(model.parameters()),
    engine=llm_engine,
    beta1=0.9,      # ä¸€é˜¶åŠ¨é‡è¡°å‡
    beta2=0.999,    # äºŒé˜¶åŠ¨é‡è¡°å‡  
    verbose=1       # æ˜¾ç¤ºè¯¦ç»†ä¼˜åŒ–è¿‡ç¨‹
)

print("\n--- åˆå§‹é¢„æµ‹ ---")
print(f"åˆå§‹ç³»ç»Ÿæç¤º: {system_prompt.value}")
prediction = model(question)
print(f"æ¨¡å‹é¢„æµ‹: {prediction.value}")

loss = eval_fn(inputs=dict(prediction=prediction, ground_truth_answer=answer))
print(f"æŸå¤±åé¦ˆ: {loss.value}\n")

print("--- TextualAdamä¸‰è½®ä¼˜åŒ–è¿‡ç¨‹ ---")

# è¿›è¡Œä¸‰è½®ä¼˜åŒ–è¿­ä»£
for iteration in range(3):
    print(f"\n=== ç¬¬ {iteration + 1} è½®ä¼˜åŒ– ===")
    
    # å‰å‘æ¨ç†å¹¶è®¡ç®—æŸå¤±
    prediction = model(question)
    loss = eval_fn(inputs=dict(prediction=prediction, ground_truth_answer=answer))
    print(f"å½“å‰é¢„æµ‹: {prediction.value}")
    print(f"å½“å‰æŸå¤±: {loss.value}")
    
    # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
    optimizer.zero_grad()
    loss.backward()
    
    # æ‰“å°æ¢¯åº¦ä¿¡æ¯
    gradient_text = system_prompt.get_gradient_text()
    print(f"ç³»ç»Ÿæç¤ºçš„æ¢¯åº¦æ•°é‡: {len(system_prompt.gradients)}")
    if gradient_text:
        print(f"æ¢¯åº¦å†…å®¹: {gradient_text}")
    else:
        print("æš‚æ— æ¢¯åº¦ä¿¡æ¯")
    
    # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
    print("æ‰§è¡Œä¼˜åŒ–æ­¥éª¤...")
    optimizer.step()
    
    print(f"ä¼˜åŒ–åç³»ç»Ÿæç¤º: {system_prompt.value}")

print("\n--- æœ€ç»ˆç»“æœå¯¹æ¯” ---")
final_prediction = model(question)
print(f"æœ€ç»ˆé¢„æµ‹: {final_prediction.value}")
final_loss = eval_fn(inputs=dict(prediction=final_prediction, ground_truth_answer=answer))
print(f"æœ€ç»ˆæŸå¤±: {final_loss.value}")

print("\n=== TextualAdamä¼˜åŒ–å™¨ç‰¹æ€§ ===")
print("âœ… ä¸€é˜¶åŠ¨é‡: è¿½è¸ªæ¢¯åº¦åé¦ˆçš„è¯­ä¹‰æ–¹å‘ä¸€è‡´æ€§")
print("âœ… äºŒé˜¶åŠ¨é‡: è¯„ä¼°æ”¹è¿›æ•ˆæœçš„å†å²æ–¹å·®") 
print("âœ… è‡ªé€‚åº”å­¦ä¹ ç‡: åŸºäºå†å²è¡¨ç°è°ƒæ•´ä¼˜åŒ–å¼ºåº¦")
print("âœ… åå·®ä¿®æ­£: é¿å…æ—©æœŸæ­¥éª¤çš„åŠ¨é‡ä¼°è®¡åå·®")
print("\nğŸ‰ TextualAdamä¼˜åŒ–å™¨æ¼”ç¤ºå®Œæˆ!")