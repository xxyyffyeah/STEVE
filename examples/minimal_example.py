import sys
import os

# ç¡®ä¿ä½¿ç”¨å½“å‰ç›®å½•çš„textgrad
current_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, current_dir)

import textgrad as tg
from textgrad.tasks import load_task
from dotenv import load_dotenv

load_dotenv(override=True)

print("=== TextualAdamä¼˜åŒ–å™¨æ¼”ç¤º ===")
print("åŸºäºBBH object countingä»»åŠ¡çš„ç³»ç»Ÿæç¤ºä¼˜åŒ–\n")

# æ£€æŸ¥APIå¯†é’¥
if not os.getenv('OPENAI_API_KEY'):
    print("è­¦å‘Š: æœªæ£€æµ‹åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
    print("è¯·è®¾ç½®APIå¯†é’¥åé‡æ–°è¿è¡Œ")
    exit(1)

llm_engine = tg.get_engine("gpt-3.5-turbo")
tg.set_backward_engine("gpt-4o")

train_set, val_set, test_set, eval_fn = load_task("BBH_object_counting", llm_engine)
question_str, answer_str = val_set[0]
question = tg.Variable(question_str, role_description="question to the LLM", requires_grad=False)
answer = tg.Variable(str(answer_str), role_description="answer to the question", requires_grad=False)

print(f"æµ‹è¯•é—®é¢˜: {question_str[:100]}...")
print(f"æ­£ç¡®ç­”æ¡ˆ: {answer_str}\n")

system_prompt = tg.Variable("You are a concise LLM. Think step by step.",
                            requires_grad=True,
                            role_description="system prompt to guide the LLM's reasoning strategy for accurate responses")

model = tg.BlackboxLLM(llm_engine, system_prompt=system_prompt)

# ä½¿ç”¨TextualAdamä¼˜åŒ–å™¨æ›¿ä»£TGD
optimizer = tg.TAdam(
    parameters=list(model.parameters()),
    engine=llm_engine,
    beta1=0.9,      # ä¸€é˜¶åŠ¨é‡è¡°å‡
    beta2=0.999,    # äºŒé˜¶åŠ¨é‡è¡°å‡  
    verbose=1       # æ˜¾ç¤ºè¯¦ç»†ä¼˜åŒ–è¿‡ç¨‹
)

print("\n--- åˆå§‹é¢„æµ‹ ---")
print(f"åˆå§‹ç³»ç»Ÿæç¤º: {system_prompt.value}")
prediction = model(question)
print(f"æ¨¡å‹é¢„æµ‹: {prediction.value}")

loss = eval_fn(inputs=dict(prediction=prediction, ground_truth_answer=answer))
print(f"æŸå¤±åé¦ˆ: {loss.value}\n")

print("--- TextualAdamä¼˜åŒ–è¿‡ç¨‹ ---")
loss.backward()
optimizer.step()

print("\n--- ä¼˜åŒ–åé¢„æµ‹ ---")
print(f"ä¼˜åŒ–åç³»ç»Ÿæç¤º: {system_prompt.value}")
prediction_after = model(question)
print(f"ä¼˜åŒ–åé¢„æµ‹: {prediction_after.value}")

print("\n=== TextualAdamä¼˜åŒ–å™¨ç‰¹æ€§ ===")
print("âœ… ä¸€é˜¶åŠ¨é‡: è¿½è¸ªæ¢¯åº¦åé¦ˆçš„è¯­ä¹‰æ–¹å‘ä¸€è‡´æ€§")
print("âœ… äºŒé˜¶åŠ¨é‡: è¯„ä¼°æ”¹è¿›æ•ˆæœçš„å†å²æ–¹å·®") 
print("âœ… è‡ªé€‚åº”å­¦ä¹ ç‡: åŸºäºå†å²è¡¨ç°è°ƒæ•´ä¼˜åŒ–å¼ºåº¦")
print("âœ… åå·®ä¿®æ­£: é¿å…æ—©æœŸæ­¥éª¤çš„åŠ¨é‡ä¼°è®¡åå·®")
print("\nğŸ‰ TextualAdamä¼˜åŒ–å™¨æ¼”ç¤ºå®Œæˆ!")