{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Improving Agents with TextGrad and TogetherAI\n",
    "\n",
    "Author: [Federico Bianchi](https://federicobianchi.io/)\n",
    "\n",
    "## Short Summary\n",
    "\n",
    "In this tutorial, we'll explore how to create self-improving AI systems using TextGrad and TogetherAI models. TextGrad is a powerful framework that enables automatic \"differentiation\" via text, allowing language models to improve themselves through textual feedback. Thanks to TogetherAI, we will be able to use TextGrad to optimize prompt from open-source models.\n",
    "\n",
    "A self-optimization framework for agents looks like this:\n",
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"https://www.agentrecipes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fevaluator-optimizer.32896758.png&w=3840&q=75&dpl=dpl_Ek3R4Pxv85QsXYQja11CAeaqGhMf\" width = 1000></div>\n",
    "\n",
    "<div align=\"center\"><i>Image from <a href=\"https://www.agentrecipes.com\">Agent Recipes</a></i></div>\n",
    "\n",
    "An LLM generate some content and then another LLM critisizes that comments and provide feedback. The feedback is used to improve the original answers. Improvement can be done N-times or until the \"evaluator\" is satisifed with the answer.\n",
    "\n",
    "There is another tutorial on looping agents from scratch form [Zain Hasan]() that might be an [interesting read]()\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this tutorial, we'll explore several practical applications of TextGrad with TogetherAI models. We'll progress through three key examples of increasing complexity:\n",
    "\n",
    "1. Test-Time Optimization - A simple warm-up to introduce the core concepts\n",
    "2. Prompt Optimization - A more sophisticated approach to enhance model performance\n",
    "3. Self-Improving Coding Agent - An advanced implementation with built-in optimization capabilities\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "* Understand the versatile applications of TextGrad for LLM optimization\n",
    "* Leverage TogetherAI's powerful models for test-time optimization\n",
    "* Implement your own optimization pipelines for various use cases\n",
    "* Apply these techniques to create more capable and efficient AI systems\n",
    "\n",
    "Each section builds kind of build on the abstractions of the previous one, but you should be able to skip without losing too much context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TextGrad?\n",
    "\n",
    "[TextGrad](https://github.com/zou-group/textgrad) is a recent framework for the end-to-end optimization of language models prompts thorugh text feedback. What this basically means is that TextGrad will allow you to optimize language models' prompts and solutions automatically.\n",
    "\n",
    "Key features of TextGrad:\n",
    "- Provides a PyTorch-inspired API for defining and optimizing text-based variables\n",
    "- Enables backpropagation of textual feedback to improve individual components\n",
    "- Works with a variety of tasks without requiring framework modifications\n",
    "- Supports optimization of diverse elements from prompts to code snippets (anything that is text)\n",
    "- Supports feedback for multimodal models\n",
    "\n",
    "A few different use-cases can be implemented in TextGrad, but the main two are:\n",
    "\n",
    "1) Prompt Optimization\n",
    "\n",
    "2) Test-Time Optimization\n",
    "\n",
    "* TextGrad will give us easy to use abstractions to build an optimization layer on top of agents or llm calls.\n",
    "* TogetherAI will give us the models to do this in an effective way!\n",
    "\n",
    "## Which Kind of \"Optimization\" is TextGrad doing?\n",
    "\n",
    "TextGrad optimizes language models using textual feedback. At the most basic level, a language model provides feedback to LLMs components (solutions, prompts) and identifies improvement opportunities. At the more sophisticated level, TextGrad offers primitives for end-to-end optimization of complex Language Model pipelines composed of multiple steps. By backpropagating feedback across different levels of LLM chains, TextGrad enables comprehensive optimization that significantly improves results.\n",
    "\n",
    "This approach allows for optimization of elements that traditional gradient-based methods cannot handle, such as discrete text structures and reasoning patterns.\n",
    "\n",
    "Fun fact: The optimization loop in TextGrad is inspired by [Karpathy's micrograd](https://github.com/karpathy/micrograd). We literally follow the same signatures to implement an AutoGrad system for Text.\n",
    "\n",
    "Here you can find an image comparing TextGrad to torch.\n",
    "\n",
    "<img src=\"https://github.com/zou-group/textgrad/raw/main/assets/analogy.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "# note that as of this time, litellm engines are still experimental\n",
    "from textgrad.engine_experimental.litellm import LiteLLMEngine\n",
    "import textgrad as tg\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "from dotenv import load_dotenv\n",
    "from textgrad.loss import MultiFieldTokenParsedEvaluation\n",
    "\n",
    "\n",
    "load_dotenv() # or make sure you have the TOGETHER_API_KEY env variable set, this is the only key we need for this tutorial\n",
    "\n",
    "# a simple support function to print text with some newlines\n",
    "def wrap_text(text, max_width=100):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "        \n",
    "    result = []\n",
    "    current_line = []\n",
    "    current_length = 0\n",
    "    \n",
    "    # Split by existing newlines first to preserve intentional line breaks\n",
    "    for line in text.split('\\n'):\n",
    "        words = line.split()\n",
    "        current_length = 0\n",
    "        current_line = []\n",
    "        \n",
    "        for word in words:\n",
    "            # If adding this word would exceed max_width\n",
    "            if current_length + len(word) + (1 if current_line else 0) > max_width:\n",
    "                # Add the current line to result and start a new line\n",
    "                result.append(' '.join(current_line))\n",
    "                current_line = [word]\n",
    "                current_length = len(word)\n",
    "            else:\n",
    "                # Add word to current line\n",
    "                if current_line:  # Add space before word if not first word\n",
    "                    current_length += 1  # Account for space\n",
    "                current_line.append(word)\n",
    "                current_length += len(word)\n",
    "        \n",
    "        # Add the last line from this original line\n",
    "        if current_line:\n",
    "            result.append(' '.join(current_line))\n",
    "    \n",
    "    return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use TogetherAI with TextGrad\n",
    "\n",
    "Thanks to LiteLLM integration, we can simply use TogetherAI models through their API. LiteLLM handles all the authentication and API calls behind the scenes (assuming you have loaded the env variables), making it very easy to use different models. In this case, we're using Meta's Llama 3 model hosted on TogetherAI's platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello. 3 + 4 is 7.'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation = LiteLLMEngine(\"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", cache=True).generate(content=\"hello, what's 3+4\", system_prompt=\"you are an assistant\")\n",
    "generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming Up Example: Test-Time Improvement\n",
    "\n",
    "So what can TextGrad do for us in practice? TextGrad can provide a layer of optimization on top of your agents, by checking their prompts and correcting them in case there is the need to do so. Let's start with an easy example.\n",
    "\n",
    "Let's assume our LLM has just generated the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution = \"\"\"To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
    "x = (-b ± √(b^2 - 4ac)) / 2a\n",
    "a = 3, b = -7, c = 2\n",
    "x = (7 ± √((-7)^2 + 4(3)(2))) / 6\n",
    "x = (7 ± √73) / 6\n",
    "The solutions are:\n",
    "x1 = (7 + √73)\n",
    "x2 = (7 - √73)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this math problem is wrong (you can check it or you can actually give this in input to an LLM which will tell you where the problem is (which is also basicall what textgrad is doing behind the secens!))\n",
    "\n",
    "Now, let's use TextGrad to improve this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = LiteLLMEngine(\"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\", cache=True)\n",
    "\n",
    "# here we set the engine that will be used to compute the gradients and update the solution\n",
    "tg.set_backward_engine(engine, override=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextGrad defines variables that will be optimized, this is similar to Torch Tensors.\n",
    "\n",
    "solution = tg.Variable(initial_solution,\n",
    "                       requires_grad=True,\n",
    "                       role_description=\"solution to the math question\")\n",
    "\n",
    "# This is the system prompt that will guide the loss function.\n",
    "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math problem. There is no reason to solve it yourself, do not give a solution to the problem, only identify errors.\"\"\",\n",
    "                                 requires_grad=False,\n",
    "                                 role_description=\"system prompt\")\n",
    "\n",
    "# This is the loss function, it will be used to compute the loss and the gradients.\n",
    "loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "\n",
    "# This is the optimizer, it will be used to update the solution. TGD stands for Textual Gradient Descent, which is an analogy to the classic gradient descent but entirely based on text.\n",
    "optimizer = tg.TGD([solution])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error in the solution is in the calculation of the discriminant (the expression inside the square root) and the final expressions for x1 and x2.\n",
      "\n",
      "The correct calculation for the discriminant is:\n",
      "(-7)^2 - 4(3)(2) = 49 - 24 = 25\n",
      "\n",
      "So, the correct expression for x is:\n",
      "x = (7 ± √25) / 6\n",
      "x = (7 ± 5) / 6\n",
      "\n",
      "And the correct solutions are:\n",
      "x1 = (7 + 5) / 6 = 12 / 6 = 2\n",
      "x2 = (7 - 5) / 6 = 2 / 6 = 1/3\n",
      "\n",
      "The given solutions x1 = (7 + √73) and x2 = (7 - √73) are incorrect because they do not have the division by 6, and the discriminant is incorrectly calculated as √73 instead of √25.\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(solution)\n",
    "print(loss.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula: \n",
      "x = (-b ± √(b^2 - 4ac)) / 2a \n",
      "a = 3, b = -7, c = 2 \n",
      "x = (7 ± √((-7)^2 - 4(3)(2))) / 6 \n",
      "x = (7 ± √(49 - 24)) / 6 \n",
      "x = (7 ± √25) / 6 \n",
      "x = (7 ± 5) / 6 \n",
      "The solutions are: \n",
      "x1 = (7 + 5) / 6 = 12 / 6 = 2 \n",
      "x2 = (7 - 5) / 6 = 2 / 6 = 1/3\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(solution.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the correct solution! You can replace the solution with anything you want to optimize. And you can optimize it multiple times in a for loop (same as you would do with Torch)\n",
    "\n",
    "There are quite a few things that TextGrad can do, mimicing Torch. For example, we can add textual contraints to the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation $3x^2 - 7x + 2 = 0$, we can use the quadratic formula: $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$. In this case, $a = 3$, $b = -7$, and $c = 2$. Plugging these values into the formula, we get: $x = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4(3)(2)}}{2(3)} = \\frac{7 \\pm \\sqrt{49 - 24}}{6} = \\frac{7 \\pm \\sqrt{25}}{6}$. Therefore, the solutions are: $x_1 = \\frac{7 + 5}{6} = \\frac{12}{6} = 2$ and $x_2 = \\frac{7 - 5}{6} = \\frac{2}{6} = \\frac{1}{3}$. Alternatively, the solutions can be written as: $x_1 = \\frac{7 + \\sqrt{25}}{6}$ and $x_2 = \\frac{7 - \\sqrt{25}}{6}$, but since $\\sqrt{25} = 5$, the first representation is more simplified. Note that the original solutions $x1 = (7 + \\sqrt{73})$ and $x2 = (7 - \\sqrt{73})$ are incorrect because they do not satisfy the given equation $3x^2 - 7x + 2 = 0$.\n"
     ]
    }
   ],
   "source": [
    "solution = tg.Variable(initial_solution,\n",
    "                       requires_grad=True,\n",
    "                       role_description=\"solution to the math question\")\n",
    "\n",
    "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math problem. There is no reason to solve it yourself, do not give a solution to the problem, only identify errors.\"\"\",\n",
    "                                 requires_grad=False,\n",
    "                                 role_description=\"system prompt\")\n",
    "                              \n",
    "loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "optimizer = tg.TGD([solution], constraints=[\"Make sure that you use latex\"])\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(wrap_text(solution.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt Optimization\n",
    "\n",
    "In this section, we'll explore how TextGrad enables prompt optimization - a powerful technique to enhance model performance without changing the model itself.\n",
    "\n",
    "\n",
    "We'll demonstrate how to optimize the system prompt for a smaller model (Llama-3.2-3B) using feedback from a much larger model (70B parameters). This approach leverages the capabilities of the larger model to guide and improve the smaller one, making it more efficient and cost-effective.\n",
    "\n",
    "For our example, we'll tackle a challenge from the Big Bench Hard (BBH) dataset - specifically the object counting task. This task presents the model with a list of various objects and tests its ability to accurately count specific items, a seemingly simple task that smaller models often struggle with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our target small model\n",
    "llm_engine_small = LiteLLMEngine(model_string=\"together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "_, val_set, _, eval_fn = load_task(\"BBH_object_counting\", llm_engine_small)\n",
    "question_str, answer_str = val_set[2]\n",
    "\n",
    "question = tg.Variable(question_str, role_description=\"question asked to the LLM\", requires_grad=False)\n",
    "answer = tg.Variable(str(answer_str), role_description=\"correct answer to the question\", requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the system prompt we want to optimize, we will start with a simple CoT prompt.\n",
    "system_prompt = tg.Variable(\"You are a concise LLM. Think step by step.\",\n",
    "                            requires_grad=True,\n",
    "                            role_description=\"system prompt to guide the LLM's reasoning strategy for accurate responses\")\n",
    "\n",
    "model = tg.BlackboxLLM(llm_engine_small, system_prompt=system_prompt)\n",
    "optimizer = tg.TGD(parameters=list(model.parameters()))\n",
    "\n",
    "prediction = model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a stalk of celery, two drums, two onions, a carrot, an accordion, a yam, a cabbage, a lettuce head, a potato, and a head of broccoli. How many vegetables do I have?\n",
      "9\n",
      "\n",
      "To find the number of vegetables, let's identify the vegetables in the list:\n",
      "\n",
      "1. Celery\n",
      "2. Onion\n",
      "3. Carrot\n",
      "4. Cabbage\n",
      "5. Lettuce\n",
      "6. Broccoli\n",
      "7. Potato\n",
      "\n",
      "There are 7 vegetables in the list.\n"
     ]
    }
   ],
   "source": [
    "print(question)\n",
    "print(answer)\n",
    "print()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "For our prompt optimization, we'll use one of TextGrad's powerful built-in loss functions.\n",
    "\n",
    "This specialized function acts like a smart evaluator, comparing our variables (like questions and answers) and providing structured feedback. Each variable gets assigned a specific \"role\" so the system understands what it's looking at - think of it as giving clear job descriptions to each piece of data.\n",
    "\n",
    "One important tip: pay attention to the order of these roles! The sequence matters significantly for how the relationships between variables are interpreted. Get this right, and TextGrad will generate precisely the feedback needed to refine your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_instruction = \"Below is a question from a question-answering task, the ground truth answer, and reasoning with the final prediction. Is the final prediction correct, i.e. the same as the ground truth answer? Say only 1 (yes) or 0 (no). Return your response within <ACCURACY> </ACCURACY> tags. e.g.<ACCURACY> 0 </ACCURACY> or <ACCURACY> 1 </ACCURACY>\"\n",
    "eval_instruction = tg.Variable(evaluation_instruction, requires_grad=False, role_description=\"evaluation instruction for the task\")\n",
    "\n",
    "role_descriptions = [\n",
    "    \"Question for the task\",\n",
    "    \"Ground truth answer\",\n",
    "    \"Reasoning and prediction from the language model\"\n",
    "]\n",
    "\n",
    "loss_fn = MultiFieldTokenParsedEvaluation(\n",
    "    eval_instruction,\n",
    "    role_descriptions=role_descriptions,\n",
    "    parse_tags=[\"<ACCURACY>\", \"</ACCURACY>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ACCURACY> 0 </ACCURACY>\n"
     ]
    }
   ],
   "source": [
    "# ok let's see the loss\n",
    "optimizer.zero_grad()\n",
    "\n",
    "loss = loss_fn([question, answer, prediction])\n",
    "print(loss.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, the loss function has just said that our result is not correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "prediction = model(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now check the new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To identify the vegetables, I will consider the characteristics and common categorizations of each item.\n",
      "\n",
      "1. Celery - a stalk of celery is a vegetable.\n",
      "2. Drums - not a vegetable, but a musical instrument.\n",
      "3. Onions - a vegetable.\n",
      "4. Onions - another instance of onions, so it's a vegetable.\n",
      "5. Carrot - a vegetable.\n",
      "6. Accordion - not a vegetable, but a musical instrument.\n",
      "7. Yam - a vegetable.\n",
      "8. Cabbage - a vegetable.\n",
      "9. Lettuce head - a vegetable.\n",
      "10. Potato - a vegetable.\n",
      "11. Broccoli - a vegetable.\n",
      "\n",
      "Counting each instance of the same item, I have:\n",
      "\n",
      "- Celery: 1\n",
      "- Onions: 2\n",
      "- Carrot: 1\n",
      "- Yam: 1\n",
      "- Cabbage: 1\n",
      "- Lettuce head: 1\n",
      "- Potato: 1\n",
      "- Broccoli: 1\n",
      "\n",
      "In total, I have 9 vegetables.\n"
     ]
    }
   ],
   "source": [
    "print(prediction.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the system prompt that helped us getting the correct result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a concise LLM. When approaching this task, ensure that you consider all items in the list,\n",
      "including multiple instances of the same item. Think critically about what constitutes a vegetable,\n",
      "and use this knowledge to inform your identification and counting. If an item could be classified in\n",
      "multiple ways, consider its most relevant characteristics or categorizations. After generating your\n",
      "list of identified vegetables, cross-check it against the original list of items to ensure that no\n",
      "vegetables were missed. Evaluate your own performance and identify areas for improvement. Analyze\n",
      "the list of items and identify any patterns or relationships that could inform your identification\n",
      "and counting of vegetables. When encountering multiple instances of the same item, ensure that you\n",
      "count each instance separately.\n"
     ]
    }
   ],
   "source": [
    "print(wrap_text(model.system_prompt.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very specific use-case and in general, you would prefer to optimize on multiple examples to avoid ending up overfitting on single\n",
    "examples. There is a full tutorial about this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents Writing Code\n",
    "\n",
    "Let's now look at the full extent of TextGrad flexibility. We will write a loss function for test-time optimization. This is one of the most advanced usages of TextGrad so a few pieces ended up being involved in this process. We'll see how we can optimize code generation directly.\n",
    "\n",
    "One of the most popular usecases nowadays is having agents writing code. However, most often these agents do not give you optimized outputs. \n",
    "\n",
    "We will simplify part of the interaction with the language model to make the example easier to follow.\n",
    "\n",
    "\n",
    "### SideNote\n",
    "Note that what we will be doing will happen completely in an unsupervised way. TextGrad supports supervised optimization (e.g., you get an llm output, you evaluate that using an external function (e.g., accuracy, and you tell the LLM that result and it will be used to optimize, you can find an example of this in another tutorial))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Evaluation Code (Plase open, read carefully)\n",
    "\n",
    "The code below contains a safety mechanism that prevents automatic execution. Before running,\n",
    " you'll need to review the code and uncomment an exception. For best practices, we recommend\n",
    "running this in a sandboxed environment like Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use below utilities to run a python function.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def run_function_in_interpreter(func_code):\n",
    "    #raise Exception(\"This function will run the code returned by GPT-4o. Remove this if you'd like to run the code!\")\n",
    "    interpreter = InteractiveShell.instance()\n",
    "    \n",
    "    interpreter.run_cell(func_code, store_history=False, silent=True)\n",
    "    \n",
    "    func_name = func_code.split(\"def \")[1].split(\"(\")[0].strip()\n",
    "    func = interpreter.user_ns[func_name]\n",
    "    \n",
    "    return func\n",
    "\n",
    "\n",
    "def test_longest_increasing_subsequence(fn):\n",
    "    nums = [10, 22, 9, 33, 21, 50, 41, 60]\n",
    "    assert fn(nums) == 5\n",
    "\n",
    "    nums = [7, 2, 1, 3, 8, 4, 9, 6, 5]\n",
    "    assert fn(nums) == 4\n",
    "\n",
    "    nums = [5, 4, 3, 2, 1]\n",
    "    assert fn(nums) == 1\n",
    "\n",
    "    nums = [1, 2, 3, 4, 5]\n",
    "    assert fn(nums) == 5\n",
    "\n",
    "    nums = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\n",
    "    assert fn(nums) == 4\n",
    "\n",
    "    nums = [10, 9, 2, 5, 3, 7, 101, 18]\n",
    "    assert fn(nums) == 4\n",
    "\n",
    "    nums = [0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15]\n",
    "    assert fn(nums) == 6\n",
    "\n",
    "    nums = [7, 7, 7, 7, 7, 7, 7]\n",
    "    assert fn(nums) == 1\n",
    "\n",
    "    nums = [20, 25, 47, 35, 56, 68, 98, 101, 212, 301, 415, 500]\n",
    "    assert fn(nums) == 11\n",
    "\n",
    "    nums = [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "    assert fn(nums) == 1\n",
    "\n",
    "    print(\"✅ All test cases passed!\")\n",
    "\n",
    "# Generate a random test case\n",
    "def generate_random_test_case(size, min_value, max_value):\n",
    "    return [random.randint(min_value, max_value) for _ in range(size)]\n",
    "\n",
    "# Test the function with a random test case\n",
    "size = 10000  # Adjust the size as needed\n",
    "min_value = 1\n",
    "max_value = 10000\n",
    "\n",
    "nums = generate_random_test_case(size, min_value, max_value)\n",
    "\n",
    "# When evaluating the code, we will run it through this function\n",
    "# this will print the results and the runtime\n",
    "def test_lis_implementation(code):\n",
    "    longest_increasing_subsequence = run_function_in_interpreter(code)\n",
    "\n",
    "    start_time = time.time()\n",
    "    lis = longest_increasing_subsequence(nums)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Test Case Size: {size}\")\n",
    "    print(f\"Longest Increasing Subsequence Length: {lis}\")\n",
    "    print(f\"Runtime: {end_time - start_time:.5f} seconds\")\n",
    "\n",
    "    # Test for all test cases\n",
    "    test_longest_increasing_subsequence(longest_increasing_subsequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_text = \"\"\"Longest Increasing Subsequence (LIS)\n",
    "\n",
    "Problem Statement:\n",
    "Given a sequence of integers, find the length of the longest subsequence that is strictly increasing. \n",
    "A subsequence is a sequence that can be derived \n",
    "from another sequence by deleting some or no elements without changing the order of the remaining elements.\n",
    "\n",
    "Input:\n",
    "The input consists of a list of integers representing the sequence.\n",
    "\n",
    "Output:\n",
    "The output should be an integer representing the length of the longest increasing subsequence.\n",
    "\n",
    "Only return the function implementation using the following format:\n",
    "\n",
    "```python\n",
    "def longest_increasing_subsequence(nums):\n",
    "[... your code ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "problem = tg.Variable(problem_text, requires_grad=False, role_description=\"problem statement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using a pretty big model here, so one would expect the code to be good!\n",
    "engine = LiteLLMEngine(\"together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\", cache=True)\n",
    "tg.set_backward_engine(engine, override=True)\n",
    "\n",
    "model = tg.BlackboxLLM(engine)\n",
    "\n",
    "answer = model(problem)\n",
    "\n",
    "first_answer = answer.value.split(\"```python\")[1].split(\"```\")[0]\n",
    "\n",
    "print(first_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this answer good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case Size: 10000\n",
      "Longest Increasing Subsequence Length: 195\n",
      "Runtime: 2.34490 seconds\n",
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "test_lis_implementation(first_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is! but maybe we can do better? this looks rather slow? Let's implement an entire textgrad pipeline this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is the variable of interest we want to optimize -- so requires_grad=True\n",
    "code = tg.Variable(value=first_answer,\n",
    "                 requires_grad=True,\n",
    "                 role_description=\"code instance to optimize\")\n",
    "\n",
    "# We are not interested in optimizing the problem -- so requires_grad=False\n",
    "problem = tg.Variable(problem_text, \n",
    "                    requires_grad=False, \n",
    "                    role_description=\"the coding problem statement\")\n",
    "\n",
    "# Let TGD know to update code!\n",
    "# Like Torch!\n",
    "optimizer = tg.TGD(parameters=[code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Losses In TextGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system prompt that will guide the behavior of the loss function.\n",
    "loss_system_prompt = \"You are a smart language model that evaluates code snippets. You do not solve problems or propose new code snippets, only evaluate existing solutions critically and strong feedback.\"\n",
    "loss_system_prompt = tg.Variable(loss_system_prompt, requires_grad=False, role_description=\"system prompt to the loss function\")\n",
    "\n",
    "# The instruction that will be the prefix\n",
    "instruction = \"\"\"Think about the problem and the code snippet. What is the runtime complexity?\"\"\"\n",
    "\n",
    "# The format string and setting up the call\n",
    "format_string = \"{instruction}\\nProblem: {{problem}}\\nCurrent Code: {{code}}\"\n",
    "format_string = format_string.format(instruction=instruction)\n",
    "\n",
    "fields = {\"problem\": None, \"code\": None}\n",
    "formatted_llm_call = tg.autograd.FormattedLLMCall(engine=engine,\n",
    "                                                  format_string=format_string,\n",
    "                                                  fields=fields,\n",
    "                                                  system_prompt=loss_system_prompt)\n",
    "\n",
    "# Finally, the loss function\n",
    "def loss_fn(problem: tg.Variable, code: tg.Variable) -> tg.Variable:\n",
    "    inputs = {\"problem\": problem, \"code\": code}\n",
    "    \n",
    "    return formatted_llm_call(inputs=inputs,\n",
    "                              response_role_description=f\"evaluation of the {code.get_role_description()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to see some magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The runtime complexity of the given code snippet is O(n^2), where n is the length of the input list `nums`.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "- The outer loop iterates over the list `nums`, which takes O(n) time.\n",
      "- The inner loop also iterates over the list `nums` up to the current index `i`, which takes O(n) time in the worst case.\n",
      "- Since the inner loop is nested within the outer loop, the total time complexity is O(n) * O(n) = O(n^2).\n",
      "\n",
      "This is not the most efficient solution for the Longest Increasing Subsequence problem, especially for large inputs. A more efficient solution would use a dynamic programming approach with a binary search, which can achieve a time complexity of O(n log n). However, the given code snippet is a straightforward and simple implementation of the dynamic programming approach, and it is correct in terms of logic and output. \n",
      "\n",
      "One minor improvement could be to add some error checking to handle the case where the input is not a list or contains non-integer values. However, this would not affect the runtime complexity. \n",
      "\n",
      "In terms of code quality, the given snippet is well-structured, readable, and follows good coding practices. The variable names are descriptive, and the comments are helpful in understanding the code's logic. The function name and docstring are also clear and concise. Overall, the code is maintainable and easy to understand.\n",
      "{Variable(value=Based on the evaluation output, the main area for improvement is the runtime complexity of the code, which is currently O(n^2). To improve this, I would suggest the following:\n",
      "\n",
      "* Consider using a more efficient algorithm, such as the binary search approach mentioned in the evaluation output, which can achieve a time complexity of O(n log n). This would involve modifying the inner loop to use a binary search instead of a linear search.\n",
      "* Another approach could be to use a data structure like a balanced binary search tree or a heap to store the lengths of the longest increasing subsequences, which would allow for faster lookup and update operations.\n",
      "* The current implementation uses a dynamic programming approach, but it's not the most efficient one. Consider using a more efficient dynamic programming approach, such as the one using a 2D array to store the lengths of the longest increasing subsequences.\n",
      "* The code currently iterates over the entire list for each element, which is not necessary. Consider using a more efficient iteration strategy, such as iterating over the list only once and using a sliding window approach to keep track of the longest increasing subsequence.\n",
      "* The code uses a simple array to store the lengths of the longest increasing subsequences, which can be slow for large inputs. Consider using a more efficient data structure, such as a numpy array or a pandas Series, which would allow for faster operations.\n",
      "\n",
      "In terms of code quality, the code is well-structured and readable, but there are a few minor improvements that could be made:\n",
      "\n",
      "* Consider adding more comments to explain the logic behind the code, especially for the inner loop.\n",
      "* The variable names are descriptive, but consider using more descriptive names for the variables, such as `longest_subsequence_length` instead of `dp`.\n",
      "* The function name and docstring are clear and concise, but consider adding more information to the docstring, such as the time complexity of the function and any assumptions made about the input.\n",
      "\n",
      "Overall, the main focus should be on improving the runtime complexity of the code, while also making minor improvements to code quality and readability.\n",
      "\n",
      "Here are some specific potential changes that could be made:\n",
      "\n",
      "* Instead of using a simple array to store the lengths of the longest increasing subsequences, consider using a data structure like a balanced binary search tree or a heap.\n",
      "* Instead of iterating over the entire list for each element, consider using a more efficient iteration strategy, such as iterating over the list only once and using a sliding window approach to keep track of the longest increasing subsequence.\n",
      "* Consider using a more efficient algorithm, such as the binary search approach mentioned in the evaluation output.\n",
      "* Consider adding more comments to explain the logic behind the code, especially for the inner loop.\n",
      "* Consider using more descriptive variable names, such as `longest_subsequence_length` instead of `dp`.\n",
      "\n",
      "These are just a few potential changes that could be made to improve the code. The key is to focus on improving the runtime complexity while also making minor improvements to code quality and readability., role=feedback to code instance to optimize, grads=set())}\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(problem, code)\n",
    "print(loss.value)\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "print(code.gradients)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case Size: 10000\n",
      "Longest Increasing Subsequence Length: 195\n",
      "Runtime: 0.00660 seconds\n",
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "test_lis_implementation(code.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So FAST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Possibilies\n",
    "\n",
    "TextGrad can be applied to many different use-cases. Some examples are described in the paper:\n",
    "\n",
    "* Long Chain Optimization\n",
    "* System Prompt Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
