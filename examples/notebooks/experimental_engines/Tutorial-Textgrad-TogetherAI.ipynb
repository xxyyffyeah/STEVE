{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Improving Agents with TextGrad and TogetherAI\n",
    "\n",
    "Author: [Federico Bianchi](https://federicobianchi.io/)\n",
    "\n",
    "## Short Summary\n",
    "\n",
    "In this cookbook, we'll explore how to create self-improving AI systems using TextGrad and TogetherAI models. TextGrad is a framework that enables automatic \"differentiation\" via text, allowing language models to improve themselves through textual feedback. Thanks to TogetherAI, we will be able to use TextGrad to optimize prompt from open-source models.\n",
    "\n",
    "A self-optimization framework for agents looks like this:\n",
    "\n",
    "\n",
    "<div align=\"center\"><img src=\"https://www.agentrecipes.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fevaluator-optimizer.32896758.png&w=3840&q=75&dpl=dpl_Ek3R4Pxv85QsXYQja11CAeaqGhMf\" width = 1000></div>\n",
    "\n",
    "<div align=\"center\"><i>Image from <a href=\"https://www.agentrecipes.com\">Agent Recipes</a></i></div>\n",
    "\n",
    "\n",
    "The first LLM generate some content and then another LLM critisizes that comments and provide feedback. The feedback is used by the first LLM to improve the its answers. Improvement can be done N-times or until the \"evaluator\" is satisifed with the answer or until a maximum number of steps is reached.\n",
    "\n",
    "\n",
    "### Goal\n",
    "\n",
    "In this tutorial, we'll explore several practical applications of TextGrad with TogetherAI models. We'll progress through three key examples of increasing complexity:\n",
    "\n",
    "1. Test-Time Optimization - A simple warm-up to introduce the core concepts. We will take an erroneous solution to a problem and ask an LLM to optimize that.\n",
    "\n",
    "2. Prompt Optimization - A more sophisticated approach to improve model performance. We will take a small model, ask it to come up with a solution to a problem - that will be incorrect - and then ask a larger LLM to improve the small model prompt so that it improves.\n",
    "\n",
    "3. Self-Improving Coding Agent - An advanced implementation with built-in optimization capabilities. We will define a more complex test-time loss that can be used to optimize Coding Agents.\n",
    "\n",
    "All these examples should be self-contained. Each section builds kind of build on the abstractions of the previous one, but you should be able to skip without losing too much context.\n",
    "\n",
    "By the end of this tutorial, you'll be able to:\n",
    "* Understand the versatile applications of TextGrad for LLM optimization\n",
    "* Leverage TogetherAI's powerful models for test-time optimization\n",
    "* Implement your own optimization pipelines for various use cases\n",
    "* Apply these techniques to create more capable and efficient AI systems\n",
    "\n",
    "\n",
    "### Other Resources\n",
    "\n",
    "There is another tutorial on looping agents for self optimization form [Zain Hasan](https://x.com/ZainHasan6) that might be an [interesting read](https://github.com/togethercomputer/together-cookbook/blob/main/Agents/Looping_Agent_Workflow.ipynb), it shows how to build some of the components you use in TextGrad from scrach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is TextGrad? ðŸ¤”\n",
    "\n",
    "[TextGrad](https://github.com/zou-group/textgrad) is a recent framework for the end-to-end optimization of language models prompts thorugh text feedback. What this basically means is that TextGrad will allow you to optimize language models' prompts and solutions automatically.\n",
    "\n",
    "Key features of TextGrad:\n",
    "- Provides a PyTorch-inspired API for defining and optimizing text-based variables\n",
    "- Enables backpropagation of textual feedback to improve individual components\n",
    "- Works with a variety of tasks without requiring framework modifications\n",
    "- Supports optimization of diverse elements from prompts to code snippets (anything that is text)\n",
    "- Supports feedback for multimodal models\n",
    "\n",
    "A few different use-cases can be implemented in TextGrad, but the main two are:\n",
    "\n",
    "1) Prompt Optimization\n",
    "\n",
    "2) Test-Time Optimization\n",
    "\n",
    "* TextGrad will give us easy to use abstractions to build an optimization layer on top of agents or llm calls.\n",
    "* TogetherAI will give us the models to do this in an effective way!\n",
    "\n",
    "## Which Kind of \"Optimization\" is TextGrad doing? \n",
    "\n",
    "TextGrad optimizes language models using textual feedback. \n",
    "\n",
    "* At the most basic level, a language model provides feedback to LLMs components (solutions, prompts) and identifies improvement opportunities. \n",
    "\n",
    "* At the more sophisticated level, TextGrad offers primitives for end-to-end optimization of complex Language Model pipelines composed of multiple steps. By backpropagating feedback across different levels of LLM chains, TextGrad enables comprehensive optimization that significantly improves results.\n",
    "\n",
    "\n",
    "## If you are curious\n",
    "\n",
    "Textgrad implements an [autograd](https://github.com/zou-group/textgrad/tree/main/textgrad/autograd) that also include some \"textual\" [algebra](https://github.com/zou-group/textgrad/tree/main/textgrad/autograd) functions. Everything in TextGrad is a variable. This means you can `tg.sum()` your variables (which run a very specific LLM operation) and backpropagate through that!\n",
    "\n",
    "Fun fact: The optimization loop in TextGrad is inspired by [Karpathy's micrograd](https://github.com/karpathy/micrograd). We literally follow the same signatures to implement an AutoGrad system for Text.\n",
    "\n",
    "Here you can find an image comparing TextGrad to torch.\n",
    "\n",
    "<img src=\"https://github.com/zou-group/textgrad/raw/main/assets/analogy.png\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textgrad\n",
    "!pip install pytorch-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/federico/.pyenv/versions/3.12.0/lib/python3.12/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from textgrad.engine_experimental.litellm import LiteLLMEngine\n",
    "import textgrad as tg\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "from dotenv import load_dotenv\n",
    "from textgrad.loss import MultiFieldTokenParsedEvaluation\n",
    "\n",
    "\n",
    "load_dotenv() # or make sure you have the TOGETHER_API_KEY env variable set, this is the only key we need for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use TogetherAI with TextGrad\n",
    "\n",
    "Thanks to LiteLLM integration, we can simply use TogetherAI models through their API. LiteLLM handles all the authentication and API calls behind the scenes (assuming you have loaded the env variables), making it very easy to use different models. In this case, we're using Meta's Llama 3 model hosted on TogetherAI's platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. The answer to 3+4 is 7.\n"
     ]
    }
   ],
   "source": [
    "# engine is one of the most basic components of TextGrad. It is a wrapper on top of llm calls that also handles caching.\n",
    "\n",
    "response = LiteLLMEngine(\"together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", cache=True).generate(content=\"hello, what's 3+4\", system_prompt=\"you are an assistant\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warming Up Example: Test-Time Improvement\n",
    "\n",
    "So what can TextGrad do for us in practice? TextGrad can provide a layer of optimization on top of your agents, by checking their prompts and outputs and correcting them in case there is the need to do so. Let's start with an easy example.\n",
    "\n",
    "Let's assume our LLM has just generated the following solution to a math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_solution = \"\"\"To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula:\n",
    "x = (-b Â± âˆš(b^2 - 4ac)) / 2a\n",
    "a = 3, b = -7, c = 2\n",
    "x = (7 Â± âˆš((-7)^2 + 4(3)(2))) / 6\n",
    "x = (7 Â± âˆš73) / 6\n",
    "The solutions are:\n",
    "x1 = (7 + âˆš73)\n",
    "x2 = (7 - âˆš73)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to this math problem is wrong (you can check it or you can actually give this in input to a large LLM. The LLM should tell you where the problem is (which is also basicall what textgrad is doing behind the secens!))\n",
    "\n",
    "Now, let's use TextGrad to improve this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = LiteLLMEngine(\"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\", cache=True)\n",
    "\n",
    "# here we set the engine that will be used to compute the gradients and update the solution.\n",
    "# think of this as the llm that is going to do the optimization.\n",
    "# this is an important step in textgrad.\n",
    "tg.set_backward_engine(engine, override=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextGrad defines variables that will be optimized, this is similar to Torch Tensors.\n",
    "# each variable has a role description that will be used to guide the optimization.\n",
    "\n",
    "# we start by defining the variable that will be optimized.\n",
    "solution = tg.Variable(initial_solution,\n",
    "                       requires_grad=True,\n",
    "                       role_description=\"solution to the math question\")\n",
    "\n",
    "# This is the system prompt that will guide the loss function (we don't want to optimize this)\n",
    "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math problem. Do not give a solution to the problem, only identify errors and describe the errors.\"\"\",\n",
    "                                 requires_grad=False,\n",
    "                                 role_description=\"system prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to define a loss function as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error in the solution is in the calculation of the discriminant (b^2 - 4ac) under the square root. \n",
      "\n",
      "The correct calculation should be:\n",
      "(-7)^2 = 49\n",
      "4(3)(2) = 24\n",
      "So, the correct expression under the square root should be:\n",
      "âˆš(49 - 24) = âˆš25\n",
      "\n",
      "The correct equation should be:\n",
      "x = (7 Â± âˆš25) / 6\n",
      "x = (7 Â± 5) / 6\n",
      "\n",
      "Additionally, the solutions x1 and x2 are missing the division by 6 and the correct calculation of the square root. The correct solutions should be:\n",
      "x1 = (7 + 5) / 6\n",
      "x2 = (7 - 5) / 6\n"
     ]
    }
   ],
   "source": [
    "# This is the loss function, it will be used to compute the loss and the gradients.\n",
    "loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "\n",
    "# This is the optimizer, it will be used to update the solution. TGD stands for Textual Gradient Descent, which is an analogy to the classic gradient descent but entirely based on text.\n",
    "optimizer = tg.TGD([solution])\n",
    "\n",
    "loss = loss_fn(solution)\n",
    "print(loss.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation 3x^2 - 7x + 2 = 0, we use the quadratic formula: \n",
      "x = (-b Â± âˆš(b^2 - 4ac)) / 2a\n",
      "where a = 3, b = -7, and c = 2.\n",
      "First, calculate the discriminant: \n",
      "(-7)^2 = 49\n",
      "4(3)(2) = 24\n",
      "So, the correct expression under the square root is:\n",
      "âˆš(49 - 24) = âˆš25\n",
      "Then, apply the quadratic formula:\n",
      "x = (7 Â± âˆš25) / 6\n",
      "Since âˆš25 = 5, the solutions can be simplified to:\n",
      "x = (7 Â± 5) / 6\n",
      "Thus, the solutions are:\n",
      "x1 = (7 + 5) / 6 = 12 / 6 = 2\n",
      "x2 = (7 - 5) / 6 = 2 / 6 = 1/3\n",
      "The solutions are x1 = 2 and x2 = 1/3.\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(solution.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was the correct solution! You can replace the solution with anything you want to optimize. \n",
    "\n",
    "And you can optimize it multiple times in a for loop (same as you would do with Torch):\n",
    "\n",
    "```python\n",
    "for i in range(5):\n",
    "    # Zero out the gradients at the start of each iteration\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(solution)\n",
    "    print(f\"Iteration {i}, Loss: {loss.value}\")\n",
    "    \n",
    "    # Compute gradients and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Updated solution:\\n{solution.value}\\n\")\n",
    "```\n",
    "\n",
    "There are quite a few things that TextGrad can do, emulating Torch. \n",
    "\n",
    "For example, we can add textual contraints to the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the equation $3x^2 - 7x + 2 = 0$, we can use the quadratic formula: $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$. In this case, $a = 3$, $b = -7$, and $c = 2$. Plugging these values into the formula, we get: $x = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4(3)(2)}}{2(3)} = \\frac{7 \\pm \\sqrt{49 - 24}}{6} = \\frac{7 \\pm \\sqrt{25}}{6}$. Therefore, the solutions are: $x_1 = \\frac{7 + 5}{6} = \\frac{12}{6} = 2$ and $x_2 = \\frac{7 - 5}{6} = \\frac{2}{6} = \\frac{1}{3}$. However, the original solution provided was $x_1 = (7 + \\sqrt{73})$ and $x_2 = (7 - \\sqrt{73})$, which seems to be incorrect based on the standard quadratic formula application. The correct calculation yields $x_1 = 2$ and $x_2 = \\frac{1}{3}$, not matching the provided solutions. Let's correct the calculation: $x = \\frac{-(-7) \\pm \\sqrt{(-7)^2 - 4 \\cdot 3 \\cdot 2}}{2 \\cdot 3} = \\frac{7 \\pm \\sqrt{49 - 24}}{6} = \\frac{7 \\pm \\sqrt{25}}{6}$, which was incorrectly calculated as $\\sqrt{73}$ instead of $\\sqrt{25}$. The correct solutions, following the proper application of the quadratic formula with the correct calculation of the discriminant ($b^2 - 4ac = (-7)^2 - 4 \\cdot 3 \\cdot 2 = 49 - 24 = 25$), are indeed $x_1 = 2$ and $x_2 = \\frac{1}{3}$. The original solutions $x_1 = (7 + \\sqrt{73})$ and $x_2 = (7 - \\sqrt{73})$ would be correct if the equation was $3x^2 - 7x + 2 = 0$ and the discriminant was $73$, but given the equation $3x^2 - 7x + 2 = 0$, the correct discriminant is $25$, leading to the solutions $x_1 = 2$ and $x_2 = \\frac{1}{3}$. Thus, the correct approach to solve $3x^2 - 7x + 2 = 0$ yields $x = \\frac{7 \\pm \\sqrt{25}}{6}$, resulting in $x_1 = \\frac{7 + 5}{6} = 2$ and $x_2 = \\frac{7 - 5}{6} = \\frac{1}{3}$, which indicates an error in the original calculation or equation setup that led to $\\sqrt{73}$.\n"
     ]
    }
   ],
   "source": [
    "solution = tg.Variable(initial_solution,\n",
    "                       requires_grad=True,\n",
    "                       role_description=\"solution to the math question\")\n",
    "\n",
    "loss_system_prompt = tg.Variable(\"\"\"You will evaluate a solution to a math problem. There is no reason to solve it yourself, do not give a solution to the problem, only identify errors.\"\"\",\n",
    "                                 requires_grad=False,\n",
    "                                 role_description=\"system prompt\")\n",
    "                              \n",
    "loss_fn = tg.TextLoss(loss_system_prompt)\n",
    "optimizer = tg.TGD([solution], constraints=[\"Make sure that you use latex\"])\n",
    "\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(solution.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt Optimization\n",
    "\n",
    "In this section, we'll explore how TextGrad enables prompt optimization - a powerful technique to enhance model performance without changing the model itself.\n",
    "\n",
    "\n",
    "We'll demonstrate how to optimize the system prompt for a smaller model (Llama-3.2-3B) using feedback from a much larger model (70B parameters). This approach leverages the capabilities of the larger model to guide and improve the smaller one, making it more efficient and cost-effective.\n",
    "\n",
    "For our example, we'll tackle a challenge from the Big Bench Hard (BBH) dataset - specifically the object counting task. This task presents the model with a list of various objects and tests its ability to accurately count specific items, a seemingly simple task that smaller models often struggle with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our target small model\n",
    "llm_engine_small = LiteLLMEngine(model_string=\"together_ai/meta-llama/Llama-3.2-3B-Instruct-Turbo\")\n",
    "\n",
    "_, val_set, _, eval_fn = load_task(\"BBH_object_counting\", llm_engine_small)\n",
    "question_str, answer_str = val_set[2]\n",
    "\n",
    "question = tg.Variable(question_str, role_description=\"question asked to the LLM\", requires_grad=False)\n",
    "answer = tg.Variable(str(answer_str), role_description=\"correct answer to the question\", requires_grad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: I have a stalk of celery, two drums, two onions, a carrot, an accordion, a yam, a cabbage, a lettuce head, a potato, and a head of broccoli. How many vegetables do I have?\n",
      "Answer: 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the system prompt we want to optimize, we will start with a simple CoT prompt.\n",
    "system_prompt = tg.Variable(\"You are a concise LLM. Think step by step.\",\n",
    "                            requires_grad=True,\n",
    "                            role_description=\"system prompt to guide the LLM's reasoning strategy for accurate responses\")\n",
    "\n",
    "# let's wrap our engine in textgrad model which is going to keep track of calls and prepare the model for backpropagation.\n",
    "model = tg.BlackboxLLM(llm_engine_small, system_prompt=system_prompt)\n",
    "optimizer = tg.TGD(parameters=list(model.parameters()))\n",
    "\n",
    "prediction = model(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the number of vegetables, let's identify the vegetables in the list:\n",
      "\n",
      "1. Celery\n",
      "2. Onion\n",
      "3. Carrot\n",
      "4. Cabbage\n",
      "5. Lettuce\n",
      "6. Potato\n",
      "7. Broccoli\n",
      "\n",
      "There are 7 vegetables in the list.\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was wrong ðŸ˜¢ let's see if we can improve this small model!\n",
    "\n",
    "### Loss\n",
    "\n",
    "For our prompt optimization, we'll use one of TextGrad's built-in loss functions.\n",
    "\n",
    "This specialized function acts like a smart evaluator, comparing our variables (like questions and answers) and providing structured feedback. Each variable gets assigned a specific \"role\" so the system understands what it's looking at - think of it as giving clear job descriptions to each piece of data.\n",
    "\n",
    "One important tip: pay attention to the order of *these roles*! The sequence matters significantly for how the relationships between variables are interpreted. Get this right, and TextGrad will generate precisely the feedback needed to refine your prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_instruction = \"\"\"Below is a question from a question-answering task, the ground truth answer, and reasoning with the final prediction.\n",
    "Is the final prediction correct, i.e. the same as the ground truth answer? Say only 1 (yes) or 0 (no). \n",
    "Return your response within <ACCURACY> </ACCURACY> tags. e.g.<ACCURACY> 0 </ACCURACY> or <ACCURACY> 1 </ACCURACY>\"\"\"\n",
    "\n",
    "eval_instruction = tg.Variable(evaluation_instruction, requires_grad=False, role_description=\"evaluation instruction for the task\")\n",
    "\n",
    "role_descriptions = [\n",
    "    \"Question for the task\",\n",
    "    \"Ground truth answer\",\n",
    "    \"Reasoning and prediction from the language model\"\n",
    "]\n",
    "\n",
    "loss_fn = MultiFieldTokenParsedEvaluation(\n",
    "    eval_instruction,\n",
    "    role_descriptions=role_descriptions,\n",
    "    parse_tags=[\"<ACCURACY>\", \"</ACCURACY>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are not there yet... <ACCURACY> 0 </ACCURACY>\n",
      "We are not there yet... <ACCURACY> 0 </ACCURACY>\n",
      "We are not there yet... <ACCURACY> 0 </ACCURACY>\n",
      "We got it!\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_fn([question, answer, prediction])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    prediction = model(question)\n",
    "\n",
    "    if \"<ACCURACY> 1 </ACCURACY>\" in loss.value:\n",
    "        print(\"We got it!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"We are not there yet... {loss.value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the system prompt that helped us getting the correct result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a concise LLM. Think step by step. Ensure to consider all types of vegetables, refer to a broad definition of vegetables, and take into account the quantity of each item. Provide a clear and concise explanation for your counting, including any assumptions or definitions used. Prioritize accuracy and completeness in your response, while maintaining conciseness where possible. Reflect on your counting process to identify potential biases or oversights and consider how your approach could be improved for future similar tasks.\n"
     ]
    }
   ],
   "source": [
    "print(model.system_prompt.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very specific use-case and in general, you would prefer to optimize on multiple examples to avoid ending up overfitting on single\n",
    "examples. There is a full tutorial about this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents Writing Code\n",
    "\n",
    "Let's now look at the full extent of TextGrad flexibility. We will write a loss function for test-time optimization. This is one of the most advanced usages of TextGrad so a few pieces ended up being involved in this process. We'll see how we can optimize code generation directly.\n",
    "\n",
    "One of the most popular usecases nowadays is having agents writing code. However, most often these agents do not give you optimized outputs. \n",
    "\n",
    "We will simplify part of the interaction with the language model to make the example easier to follow.\n",
    "\n",
    "\n",
    "### SideNote\n",
    "Note that what we will be doing will happen completely in an unsupervised way. TextGrad supports supervised optimization (e.g., you get an llm output, you evaluate that using an external function (e.g., accuracy, and you tell the LLM that result and it will be used to optimize, you can find an example of this in another tutorial))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Evaluation Code (Plase open, read carefully)\n",
    "\n",
    "The code below contains a safety mechanism that prevents automatic execution. Before running,\n",
    " you'll need to review the code and uncomment an exception. For best practices, we recommend\n",
    "running this in a sandboxed environment like Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use below utilities to run a python function.\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def run_function_in_interpreter(func_code):\n",
    "    #raise Exception(\"This function will run the code returned by GPT-4o. Remove this if you'd like to run the code!\")\n",
    "    interpreter = InteractiveShell.instance()\n",
    "    \n",
    "    interpreter.run_cell(func_code, store_history=False, silent=True)\n",
    "    \n",
    "    func_name = func_code.split(\"def \")[1].split(\"(\")[0].strip()\n",
    "    func = interpreter.user_ns[func_name]\n",
    "    \n",
    "    return func\n",
    "\n",
    "\n",
    "def test_longest_increasing_subsequence(fn):\n",
    "    nums = [10, 22, 9, 33, 21, 50, 41, 60]\n",
    "    assert fn(nums) == 5\n",
    "\n",
    "    nums = [7, 2, 1, 3, 8, 4, 9, 6, 5]\n",
    "    assert fn(nums) == 4\n",
    "\n",
    "    nums = [5, 4, 3, 2, 1]\n",
    "    assert fn(nums) == 1\n",
    "\n",
    "    nums = [1, 2, 3, 4, 5]\n",
    "    assert fn(nums) == 5\n",
    "\n",
    "    nums = [3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]\n",
    "    assert fn(nums) == 4\n",
    "\n",
    "    nums = [10, 9, 2, 5, 3, 7, 101, 18]\n",
    "    assert fn(nums) == 4\n",
    "\n",
    "    nums = [0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15]\n",
    "    assert fn(nums) == 6\n",
    "\n",
    "    nums = [7, 7, 7, 7, 7, 7, 7]\n",
    "    assert fn(nums) == 1\n",
    "\n",
    "    nums = [20, 25, 47, 35, 56, 68, 98, 101, 212, 301, 415, 500]\n",
    "    assert fn(nums) == 11\n",
    "\n",
    "    nums = [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "    assert fn(nums) == 1\n",
    "\n",
    "    print(\"âœ… All test cases passed!\")\n",
    "\n",
    "# Generate a random test case\n",
    "def generate_random_test_case(size, min_value, max_value):\n",
    "    return [random.randint(min_value, max_value) for _ in range(size)]\n",
    "\n",
    "# Test the function with a random test case\n",
    "size = 10000  # Adjust the size as needed\n",
    "min_value = 1\n",
    "max_value = 10000\n",
    "\n",
    "nums = generate_random_test_case(size, min_value, max_value)\n",
    "\n",
    "# When evaluating the code, we will run it through this function\n",
    "# this will print the results and the runtime\n",
    "def test_lis_implementation(code):\n",
    "    longest_increasing_subsequence = run_function_in_interpreter(code)\n",
    "\n",
    "    start_time = time.time()\n",
    "    lis = longest_increasing_subsequence(nums)\n",
    "    end_time = time.time()\n",
    "\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    print(f\"Test Case Size: {size}\")\n",
    "    print(f\"Longest Increasing Subsequence Length: {lis}\")\n",
    "    print(f\"Runtime: {runtime:.5f} seconds\")\n",
    "\n",
    "    # Test for all test cases\n",
    "    test_longest_increasing_subsequence(longest_increasing_subsequence)\n",
    "\n",
    "    return runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_text = \"\"\"Longest Increasing Subsequence (LIS)\n",
    "\n",
    "Problem Statement:\n",
    "Given a sequence of integers, find the length of the longest subsequence that is strictly increasing. \n",
    "A subsequence is a sequence that can be derived \n",
    "from another sequence by deleting some or no elements without changing the order of the remaining elements.\n",
    "\n",
    "Input:\n",
    "The input consists of a list of integers representing the sequence.\n",
    "\n",
    "Output:\n",
    "The output should be an integer representing the length of the longest increasing subsequence.\n",
    "\n",
    "Only return the function implementation using the following format:\n",
    "\n",
    "```python\n",
    "def longest_increasing_subsequence(nums):\n",
    "[... your code ...]\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "problem = tg.Variable(problem_text, requires_grad=False, role_description=\"problem statement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def longest_increasing_subsequence(nums):\n",
      "    \"\"\"\n",
      "    This function calculates the length of the longest increasing subsequence in a given list of integers.\n",
      "\n",
      "    Args:\n",
      "    nums (list): A list of integers.\n",
      "\n",
      "    Returns:\n",
      "    int: The length of the longest increasing subsequence.\n",
      "    \"\"\"\n",
      "    \n",
      "    # If the input list is empty, return 0\n",
      "    if not nums:\n",
      "        return 0\n",
      "    \n",
      "    # Initialize a list to store the lengths of the longest increasing subsequences ending at each position\n",
      "    dp = [1] * len(nums)\n",
      "    \n",
      "    # Iterate over the input list\n",
      "    for i in range(1, len(nums)):\n",
      "        # For each element, compare it with all previous elements\n",
      "        for j in range(i):\n",
      "            # If the current element is greater than the previous element, update the length of the subsequence\n",
      "            if nums[i] > nums[j]:\n",
      "                dp[i] = max(dp[i], dp[j] + 1)\n",
      "    \n",
      "    # Return the maximum length found\n",
      "    return max(dp)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are using a pretty big model here, so one would expect the code to be good!\n",
    "# we are also overriding the backward engine\n",
    "engine = LiteLLMEngine(\"together_ai/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\", cache=True)\n",
    "tg.set_backward_engine(engine, override=True)\n",
    "\n",
    "model = tg.BlackboxLLM(engine)\n",
    "\n",
    "answer = model(problem)\n",
    "\n",
    "first_answer = answer.value.split(\"```python\")[1].split(\"```\")[0]\n",
    "\n",
    "print(first_answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this answer good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case Size: 10000\n",
      "Longest Increasing Subsequence Length: 192\n",
      "Runtime: 2.41174 seconds\n",
      "âœ… All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "test_lis_implementation(first_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is! but maybe we can do better? this looks rather slow? Let's implement an entire textgrad pipeline this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code is the variable of interest we want to optimize -- so requires_grad=True\n",
    "code = tg.Variable(value=first_answer,\n",
    "                 requires_grad=True,\n",
    "                 role_description=\"code instance to optimize\")\n",
    "\n",
    "# We are not interested in optimizing the problem -- so requires_grad=False\n",
    "problem = tg.Variable(problem_text, \n",
    "                    requires_grad=False, \n",
    "                    role_description=\"the coding problem statement\")\n",
    "\n",
    "# Let TGD know to update code!\n",
    "optimizer = tg.TGD(parameters=[code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Losses In TextGrad\n",
    "\n",
    "This is how you write a full new loss in TextGrad, it requires putting together a few details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system prompt that will guide the behavior of the loss function.\n",
    "loss_system_prompt = \"You are a smart language model that evaluates code snippets. You do not solve problems or propose new code snippets, only evaluate existing solutions critically and strong feedback.\"\n",
    "loss_system_prompt = tg.Variable(loss_system_prompt, requires_grad=False, role_description=\"system prompt to the loss function\")\n",
    "\n",
    "# The instruction that will be the prefix\n",
    "instruction = \"\"\"Think about the problem and the code snippet. What is the runtime complexity?\"\"\"\n",
    "\n",
    "# The format string and setting up the call\n",
    "format_string = \"{instruction}\\nProblem: {{problem}}\\nCurrent Code: {{code}}\"\n",
    "format_string = format_string.format(instruction=instruction)\n",
    "\n",
    "fields = {\"problem\": None, \"code\": None}\n",
    "formatted_llm_call = tg.autograd.FormattedLLMCall(engine=engine,\n",
    "                                                  format_string=format_string,\n",
    "                                                  fields=fields,\n",
    "                                                  system_prompt=loss_system_prompt)\n",
    "\n",
    "# The loss function\n",
    "def loss_fn(problem: tg.Variable, code: tg.Variable) -> tg.Variable:\n",
    "    inputs = {\"problem\": problem, \"code\": code}\n",
    "    \n",
    "    return formatted_llm_call(inputs=inputs,\n",
    "                              response_role_description=f\"evaluation of the {code.get_role_description()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to see some magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case Size: 10000\n",
      "Longest Increasing Subsequence Length: 194\n",
      "Runtime: 2.37487 seconds\n",
      "âœ… All test cases passed!\n",
      "We are not there yet... 2.3748748302459717\n",
      "Test Case Size: 10000\n",
      "Longest Increasing Subsequence Length: 194\n",
      "Runtime: 2.40329 seconds\n",
      "âœ… All test cases passed!\n",
      "We are not there yet... 2.4032862186431885\n",
      "Test Case Size: 10000\n",
      "Longest Increasing Subsequence Length: 194\n",
      "Runtime: 1.12705 seconds\n",
      "âœ… All test cases passed!\n",
      "We got it!\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    runtime = test_lis_implementation(code.value)\n",
    "    if runtime < 1.5:\n",
    "        print(\"We got it!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"We are not there yet... {runtime}\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_fn(problem, code)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got faster code! Let's see it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def longest_increasing_subsequence(nums):\n",
      "    \"\"\"\n",
      "    This function calculates the length of the longest increasing subsequence in a given list of integers.\n",
      "\n",
      "    Args:\n",
      "    nums (list): A list of integers.\n",
      "\n",
      "    Returns:\n",
      "    int: The length of the longest increasing subsequence.\n",
      "\n",
      "    Raises:\n",
      "    ValueError: If the input list contains non-integer values.\n",
      "    \"\"\"\n",
      "\n",
      "    # Check if the input list contains only integers\n",
      "    if not all(isinstance(num, int) for num in nums):\n",
      "        raise ValueError(\"Input list must contain only integers, but found non-integer value\")\n",
      "\n",
      "    # If the input list is empty, return 0\n",
      "    if not nums:\n",
      "        return 0\n",
      "\n",
      "    # Initialize a list to store the lengths of the longest increasing subsequences ending at each position\n",
      "    lengths = [1] * len(nums)\n",
      "\n",
      "    # Initialize a list to store the previous element in the longest increasing subsequence ending at each position\n",
      "    prev_elements = [None] * len(nums)\n",
      "\n",
      "    # Iterate over the input list\n",
      "    for i in range(1, len(nums)):\n",
      "        # For each element, compare it with all previous elements\n",
      "        for j in range(i):\n",
      "            # If the current element is greater than the previous element, update the length of the subsequence\n",
      "            if nums[i] > nums[j] and lengths[i] < lengths[j] + 1:\n",
      "                lengths[i] = lengths[j] + 1\n",
      "                prev_elements[i] = j\n",
      "\n",
      "    # Find the index of the maximum length\n",
      "    max_length_idx = max(range(len(lengths)), key=lambda i: lengths[i])\n",
      "\n",
      "    # Reconstruct the longest increasing subsequence\n",
      "    subsequence = []\n",
      "    while max_length_idx is not None:\n",
      "        subsequence.append(nums[max_length_idx])\n",
      "        max_length_idx = prev_elements[max_length_idx]\n",
      "\n",
      "    # Return the length of the longest increasing subsequence\n",
      "    return len(subsequence)\n"
     ]
    }
   ],
   "source": [
    "print(code.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Possibilies\n",
    "\n",
    "TextGrad can be applied to many different use-cases. Some examples are described in the paper:\n",
    "\n",
    "* Long Chain Optimization\n",
    "* System Prompt Optimization\n",
    "* Molecule Optimization\n",
    "* Radioteraphy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
